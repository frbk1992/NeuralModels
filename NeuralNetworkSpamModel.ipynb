{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import csv\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import matthews_corrcoef, confusion_matrix\n",
    "from time import time\n",
    "import random\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes = [\"spam\", \"ham\"]\n",
    "\n",
    "def load_spam_data():\n",
    "\n",
    "    # Data File\n",
    "    file = \"spam.csv\"\n",
    "\n",
    "    # Lists to store all word frequencies from messages\n",
    "    # used to find top words for each class\n",
    "    content_spam = {}\n",
    "    content_ham = {}\n",
    "\n",
    "    # Lists for each message and corresponding label\n",
    "    messages = []\n",
    "    y = []\n",
    "    \n",
    "    amount_spam = 0\n",
    "    amount_ham = 0\n",
    "\n",
    "    # open the file for processing as a CSV\n",
    "    with open(file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "\n",
    "        for i, row in enumerate(reader):\n",
    "            if i == 0:\n",
    "                continue\n",
    "\n",
    "            # split the string and remove all non alpha characters (or ')\n",
    "            words = [''.join(c for c in word if c.isalpha() or c == \"'\") for word in row[1].lower().split()]\n",
    "            \n",
    "            if row[0] == \"spam\":\n",
    "                amount_spam+=1\n",
    "            else:\n",
    "                amount_ham+=1\n",
    "            \n",
    "            # Add and count words for spam and ham classes\n",
    "            content = content_spam if row[0] == \"spam\" else content_ham\n",
    "            for w in words:\n",
    "                if len(w) > 3:\n",
    "                    if w in content:\n",
    "                        content[w] += 1\n",
    "                    else:\n",
    "                        content[w] = 1\n",
    "\n",
    "            # Append full messages\n",
    "            messages.append(\" \".join(words))\n",
    "            y.append(classes.index(row[0]))\n",
    "\n",
    "    # sort each each word based on value count\n",
    "    sorted_X_spam = sorted(content_spam, key=content_spam.get, reverse=True)\n",
    "    sorted_X_ham = sorted(content_ham, key=content_ham.get, reverse=True)\n",
    "    print('amount_spam ', amount_spam)\n",
    "    print('amount_ham ', amount_ham)\n",
    "    # populate the bag-of-words with top 50 words from each class (and remove duplicates)\n",
    "    bow = []\n",
    "    for i in range(80):\n",
    "        if sorted_X_spam[i] not in bow:\n",
    "            bow.append(sorted_X_spam[i])\n",
    "        if sorted_X_ham[i] not in bow:\n",
    "            bow.append(sorted_X_ham[i])\n",
    "\n",
    "\n",
    "    return bow, messages, np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amount_spam  747\n",
      "amount_ham  4825\n"
     ]
    }
   ],
   "source": [
    "bow, messages, y = load_spam_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get the vector X \n",
    "X = CountVectorizer(vocabulary=bow).fit_transform(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2073c3e5128>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAV0AAAD8CAYAAADUv3dIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqUE2upLUIBtdSWqQja4kNchGV5Ia9P9o8XEp5RsxWwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x2073c3d9160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[:100].toarray(), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get a random training and testing set\n",
    "X_train, X_test, y_train2, y_test2 = train_test_split(X, y, test_size=0.33, shuffle=True, random_state=42)\n",
    "\n",
    "#y is a vector, it has to be a matrix, this loop creates a matrix size y.shape[0]x2\n",
    "y2 = np.zeros((y.shape[0], 2))\n",
    "for i in range(y.shape[0]):\n",
    "    y2[i, 0] = y[i]\n",
    "    if y[i] == 0:\n",
    "        y2[i, 1] = 1\n",
    "    else:\n",
    "        y2[i, 1] = 0\n",
    "y = y2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modify the type from np.float64 (default type) to np.float32\n",
    "X = X.A.astype(np.float32)\n",
    "X_train = X_train.A.astype(np.float32)\n",
    "y_train2 = y_train2.astype(np.int32)\n",
    "X_test = X_test.A.astype(np.float32)\n",
    "y_test2 = y_test2.astype(np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "747\n",
      "4825\n",
      "(3733, 141)\n",
      "(3733, 2)\n",
      "(1839, 141)\n",
      "(1839, 2)\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "b = 0\n",
    "y_train = np.zeros((y_train2.shape[0], 2))\n",
    "for i in range(y_train2.shape[0]):\n",
    "    y_train[i, 0] = y_train2[i]\n",
    "    if y_train2[i] == 0:\n",
    "        a += 1\n",
    "        y_train[i, 1] = 1\n",
    "    else:\n",
    "        y_train[i, 1] = 0\n",
    "        b += 1\n",
    "        \n",
    "y_test = np.zeros((y_test2.shape[0], 2))\n",
    "for i in range(y_test2.shape[0]):\n",
    "    y_test[i, 0] = y_test2[i]\n",
    "    if y_test2[i] == 0:\n",
    "        a += 1 \n",
    "        y_test[i, 1] = 1\n",
    "    else:\n",
    "        y_test[i, 1] = 0\n",
    "        b += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model (this should work without any change)\n",
    "def neural_net(x, num_input, weights, biases):\n",
    "    x_matrix = tf.reshape(x, [-1, num_input])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 = tf.add(tf.matmul(x_matrix, weights['h1']), biases['b1'])\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.nn.softmax(tf.matmul(layer_2, weights['out']) + biases['out'])\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model (this should work without any change)\n",
    "def create_model_train_test(X_train, y_train, X_test, y_test, neurons, lr, bz):\n",
    "    \n",
    "    # Parameters for the classifier\n",
    "    learning_rate = lr\n",
    "    batch_size = bz #the classifier will take blocks of 128 to run the classifier\n",
    "    display_step = 5\n",
    "    num_steps = int(X_train.shape[0]/batch_size) # 58 = ceil(7406/128) = ceil(57.85)\n",
    "\n",
    "\n",
    "    # Network Parameters\n",
    "    n_hidden_1 = neurons # 1st layer number of neurons\n",
    "    n_hidden_2 = neurons # 2nd layer number of neurons\n",
    "    num_input = X_train.shape[1] # number of features\n",
    "    num_classes = y_train.shape[1] # total amount of classes\n",
    "\n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(tf.float32, [None], name=\"input\")\n",
    "    Y = tf.placeholder(tf.int32, [None, num_classes], name=\"output\")\n",
    "    \n",
    "    # Store layers weight & bias (this should work without any change)\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "        'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "        'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "        'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "    }\n",
    "    \n",
    "    logits = neural_net(X, num_input, weights, biases)\n",
    "    sofmax_out = tf.nn.softmax(logits)\n",
    "\n",
    "    # Define loss and optimizer\n",
    "    loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "    # Evaluate model (with test logits, for dropout to be disabled)\n",
    "    correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "    # Initialize the variables (i.e. assign their default value)\n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        # Run the initializer\n",
    "        sess.run(init)\n",
    "\n",
    "        for i in range(1, num_steps+1):\n",
    "            batch_x = X_train[(i-1)*batch_size: (i-1)*batch_size + batch_size - 1] #take a block of 128 instances\n",
    "            batch_y = y_train[(i-1)*batch_size: (i-1)*batch_size + batch_size - 1] #take a block of 128 instances\n",
    "            batch_x = batch_x.ravel()\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "\n",
    "        y_out = sess.run(sofmax_out, feed_dict={X: X_test.ravel()})\n",
    "        acc =  np.mean((y_test - y_out.round()) == 0)\n",
    "        mcc = matthews_corrcoef(y_test[:,0], y_out.round()[:,0])\n",
    "        print('with neurons', neurons, ' bz', bz, ' lr', lr,  \\\n",
    "              ' acc = ',acc, ' mcc = ', mcc)\n",
    "\n",
    "\n",
    "        #this variables are used for the binary model, catch the current state for every weight and every bias\n",
    "        W1 = weights['h1'].eval(sess)\n",
    "        B1 = biases['b1'].eval(sess)\n",
    "        W2 = weights['h2'].eval(sess)\n",
    "        B2 = biases['b2'].eval(sess)\n",
    "        W_OUT = weights['out'].eval(sess)\n",
    "        B_OUT = biases['out'].eval(sess)\n",
    "\n",
    "        sess.close()\n",
    "        del sess\n",
    "    \n",
    "    return W1, B1, W2, B2, W_OUT, B_OUT, mcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "64\n",
      "128\n",
      "192\n",
      "256\n",
      "320\n",
      "384\n",
      "448\n",
      "<class 'enumerate'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X))\n",
    "neurons = [64,128,192,256,320,384,448]\n",
    "for neuron in neurons:\n",
    "    print(neuron)\n",
    "print(type(enumerate(neurons)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------bz  32 --------\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.915770609319  mcc =  0.492955373195\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.931899641577  mcc =  0.685782875433\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.915619389587  mcc =  0.599475104903\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.924596050269  mcc =  0.621225456338\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.913824057451  mcc =  0.557129928708\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.901256732496  mcc =  0.627558565165\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.877917414722  mcc =  0.245593781224\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.874326750449  mcc =  0.354089879856\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.933572710952  mcc =  0.654760351104\n",
      "with neurons 32  bz 16  lr 0.01  acc =  0.858168761221  mcc =  0.177111036072\n",
      "mean mcc  0.5015682352\n",
      "------bz  64 --------\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.883512544803  mcc =  0.164672961221\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.910394265233  mcc =  0.603220084236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:538: RuntimeWarning: invalid value encountered in true_divide\n",
      "  mcc = cov_ytyp / np.sqrt(cov_ytyt * cov_ypyp)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with neurons 64  bz 16  lr 0.01  acc =  0.874326750449  mcc =  0.0\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.858168761221  mcc =  0.0\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.883303411131  mcc =  0.354257880786\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.906642728905  mcc =  0.48915524077\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.903052064632  mcc =  0.541464179055\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.879712746858  mcc =  0.220759505385\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.856373429084  mcc =  0.199023173642\n",
      "with neurons 64  bz 16  lr 0.01  acc =  0.876122082585  mcc =  0.218992217261\n",
      "mean mcc  0.279154524236\n",
      "------bz  128 --------\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.874551971326  mcc =  0.11097054237\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.874551971326  mcc =  0.341065996987\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.797127468582  mcc =  0.300663584202\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.865350089767  mcc =  0.106691137702\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.876122082585  mcc =  -0.0158147609257\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.910233393178  mcc =  0.633807500563\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.854578096948  mcc =  0.170695360643\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.881508078995  mcc =  0.322024861426\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.886894075404  mcc =  0.355406974858\n",
      "with neurons 128  bz 16  lr 0.01  acc =  0.840215439856  mcc =  0.0\n",
      "mean mcc  0.232551119782\n",
      "------bz  256 --------\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.876344086022  mcc =  -0.0157844269209\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.89247311828  mcc =  -0.0314426968642\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.874326750449  mcc =  0.155802035527\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.83842010772  mcc =  0.0\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.840215439856  mcc =  0.0\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.881508078995  mcc =  0.140351311671\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.890484739677  mcc =  0.367173425844\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.868940754039  mcc =  0.212345316257\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.872531418312  mcc =  0.0\n",
      "with neurons 256  bz 16  lr 0.01  acc =  0.845601436266  mcc =  0.0\n",
      "mean mcc  0.0828444965514\n",
      "best mcc  0.685782875433\n",
      "where the mcc is better in mccs  0\n",
      "best value in mccs  0.5015682352\n",
      "highest neurons 32\n",
      "132.21042585372925\n"
     ]
    }
   ],
   "source": [
    "#Test the model with different nomber of neurons to find the most optimal value\n",
    "#\n",
    "t1 = time()\n",
    "k = 0\n",
    "mcc = -2\n",
    "mccs = np.zeros(9*10).reshape(9,10)\n",
    "neurons = [32,64,128,256]\n",
    "bz = [1,4,8,16,32,64,128]\n",
    "split = 10\n",
    "mccs = np.zeros(len(neurons)*split).reshape(len(neurons),split)\n",
    "for i, neuron in enumerate(neurons):\n",
    "    print('------bz ', neuron, '--------')\n",
    "    kf = KFold(n_splits=split, shuffle=True, random_state=random.randint(1,100))\n",
    "    k = 0\n",
    "    for train, test in kf.split(X):\n",
    "        X_train, X_test, y_train, y_test = X[train], X[test], y[train], y[test]\n",
    "        #print((y_train.shape))\n",
    "        mcc_old = mcc\n",
    "        W1, B1, W2, B2, W_OUT, B_OUT, mcc = create_model_train_test(X_train, y_train, X_test, y_test, neuron, 0.01, 16)\n",
    "        W1_old, B1_old, W2_old, B2_old, W_OUT_old, B_OUT_old = W1, B1, W2, B2, W_OUT, B_OUT\n",
    "        mccs[i][k] = mcc\n",
    "        k += 1\n",
    "        if(mcc != -2):\n",
    "            if(mcc < mcc_old):\n",
    "                mcc = mcc_old\n",
    "                W1, B1, W2, B2, W_OUT, B_OUT = W1_old, B1_old, W2_old, B2_old, W_OUT_old, B_OUT_old\n",
    "    \n",
    "    print('mean mcc ',mccs.mean(axis=1)[i])\n",
    "            \n",
    "print('best mcc ', mcc)\n",
    "print('where the mcc is better in mccs ',np.argmax(mccs.mean(axis=1)))\n",
    "print('best value in mccs ',mccs.mean(axis=1)[np.argmax(mccs.mean(axis=1))])\n",
    "print('highest neurons', neurons[np.argmax(mccs.mean(axis=1))])\n",
    "print(time() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with neurons 8  bz 4  lr 0.01  acc =  0.927678085916  mcc =  0.665490090076\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.941272430669  mcc =  0.709281989101\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.932572050027  mcc =  0.666444315258\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.925502990756  mcc =  0.648714118212\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.927134312126  mcc =  0.639895973733\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.908646003263  mcc =  0.572176337908\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.933659597607  mcc =  0.686576598324\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.920609026645  mcc =  0.621277980041\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.935834692768  mcc =  0.702228873111\n",
      "with neurons 8  bz 4  lr 0.01  acc =  0.917890157694  mcc =  0.585025897371\n",
      "best mcc  0.709281989101\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#W1, B1, W2, B2, W_OUT, B_OUT, mcc = create_model_train_test(X_train, y_train, X_test, y_test, 296, 0.001, 3)\n",
    "\n",
    "mcc = -2\n",
    "mcc_old = mcc\n",
    "for i in range(10):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=random.randint(1,100))\n",
    "    mcc_old = mcc\n",
    "    W1, B1, W2, B2, W_OUT, B_OUT, mcc = create_model_train_test(X_train, y_train, X_test, y_test, 8, 0.01, 4)\n",
    "    W1_old, B1_old, W2_old, B2_old, W_OUT_old, B_OUT_old = W1, B1, W2, B2, W_OUT, B_OUT\n",
    "    if(mcc != -2):\n",
    "        if(mcc < mcc_old):\n",
    "            mcc = mcc_old\n",
    "            W1, B1, W2, B2, W_OUT, B_OUT = W1_old, B1_old, W2_old, B2_old, W_OUT_old, B_OUT_old\n",
    "\n",
    "            \n",
    "print('best mcc ', mcc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#create the model\n",
    "DIR = '/sms_model/' # path where the model will be create it\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # this steps are the same as the function neural_net() but instead of random values will \n",
    "    # take the value in every weight and every bias at the end of the classifier\n",
    "    \n",
    "    #create the input parameter which is a vector\n",
    "    x_2 = tf.placeholder(tf.float32, shape=[None], name=\"input\")\n",
    "    \n",
    "    #reshape the vector into a matrix, this has to be done because the the op matmul\n",
    "    #requires two matrix's\n",
    "    x_22 = tf.reshape(x_2, [-1, 141])\n",
    "    \n",
    "    #create each weight and each biase with the final value\n",
    "    W1_C = tf.constant(W1, name=\"W1\")\n",
    "    B1_C = tf.constant(B1, name=\"B1\")\n",
    "    W2_C = tf.constant(W2, name=\"W2\")\n",
    "    B2_C = tf.constant(B2, name=\"B2\")\n",
    "    W_OUT_C = tf.constant(W_OUT, name=\"W_OUT\")\n",
    "    B_OUT_C = tf.constant(B_OUT, name=\"B_OUT\")\n",
    "    \n",
    "    \n",
    "    # the two layers as used in neural_net()\n",
    "    layer_1 = tf.add(tf.matmul(x_22, W1_C, name=\"matmul_x_22_w1_c\"), B1_C, name=\"add_matmul1_b1_c\")\n",
    "    layer_2 = tf.add(tf.matmul(layer_1, W2_C, name=\"matmul_l_1_w1_c\"), B2_C, name=\"add_matmul2_b2_c\")\n",
    "    #the output value of the classifier with the name output\n",
    "    OUTPUT = tf.nn.softmax(tf.matmul(layer_2, W_OUT, name=\"matmul_l_2_wout_c\") + B_OUT, name=\"output\")\n",
    "    \n",
    "\n",
    "    # skipped dropout for exported graph as there is no need for already calculated weights\n",
    "    sess = tf.Session()\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "\n",
    "    graph_def = g.as_graph_def()\n",
    "    \n",
    "    #create the model model_graph.pb\n",
    "    tf.train.write_graph(graph_def, DIR, 'sms_model_opt5.pb', as_text=False) #check accuracy 0.910277 in 1\n",
    "\n",
    "    \n",
    "    # Test trained model\n",
    "    y_train2 = tf.placeholder(tf.float32, [None, 2])\n",
    "    correct_prediction = tf.equal(tf.argmax(OUTPUT, 1), tf.argmax(y_train2, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    #print(\"check accuracy %g\" % accuracy.eval({x_2: X_test.ravel(), y_train: y_test}, sess))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
